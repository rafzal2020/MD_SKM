{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612841af",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "# **Streaming K‐Means Clustering Pipeline**\n",
    "\n",
    "**Author**: Vraj Shah\n",
    "\n",
    "</div>\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this assignment is to build and evaluate a **Streaming K‐Means** clustering pipeline using PySpark. We will first review the standard (batch) K‐Means algorithm—its assumptions, steps, and limitations—then introduce the streaming variant, which processes data in mini‐batches and incrementally updates cluster centroids. By the end of this exercise, you should understand:\n",
    "\n",
    "1. How standard K‐Means partitions a fixed dataset into k clusters by iteratively updating centroids to minimize within‐cluster variance.\n",
    "2. Why standard K‐Means does not scale to unbounded data streams or handle concept drift.\n",
    "3. How **Streaming K‐Means** (a weighted incremental version) updates centroids on each incoming micro‐batch, using a decay factor to “forget” old data gradually.\n",
    "4. The differences in initialization, update rules, and evaluation between batch and streaming K‐Means.\n",
    "\n",
    "### Standard (Batch) K‐Means\n",
    "\n",
    "- **Problem setup:** \n",
    "  Given a fixed dataset $X = \\{x_1, \\dots, x_n\\}$ in $\\mathbb{R}^d$ and a user-defined number of clusters $k$, the goal is to find centroids $\\{\\mu_1, \\dots, \\mu_k\\}$ that minimize the within-cluster sum of squared distances:  \n",
    "  $$\n",
    "  \\text{Objective:} \\quad \\sum_{i=1}^n \\min_{j \\in \\{1,\\dots,k\\}} \\| x_i - \\mu_j \\|^2\n",
    "  $$\n",
    "- **Algorithm steps:**\n",
    "  1. **Initialization:** Choose \\(k\\) initial centroids (e.g., via random sampling or K‐Means++).  \n",
    "  2. **Assignment:** Assign each point $x_i$ to the nearest centroid $\\mu_j$ .  \n",
    "  3. **Update:** For each cluster \\(j\\), recompute the centroid as the mean of all points assigned to \\(j\\).  \n",
    "  4. **Iterate:** Repeat assignment and update until convergence (centroids stabilize or max iterations reached).  \n",
    "- **Limitations:**  \n",
    "  - Requires multiple passes over the entire dataset—impractical for data that arrives continuously.  \n",
    "  - Cannot adapt if the underlying distribution shifts over time (“concept drift”).  \n",
    "  - If the dataset is too large to fit in memory, you need multiple distributed passes, which is expensive.\n",
    "\n",
    "### Streaming K‐Means\n",
    "\n",
    "- **Setup:** Data arrives in a stream (e.g., from Kafka, file source, or a continuously growing RDD). We break the stream into **micro‐batches** of size \\(b\\) (e.g., 1000 points).  \n",
    "- **Key idea:** \n",
    "Maintain $k$ centroids $\\mu_j^{(t)}$ and effective counts $N_j^{(t)}$ at time $t$. For each incoming batch $B_t$:\n",
    "1. **Compute batch statistics** – Assign each point to its closest centroid and compute sum of vectors and count per cluster  \n",
    "2. **Update centroids** – Use exponential decay to merge new stats with past values using a decay factor $\\lambda \\in [0, 1]$.\n",
    "3. **Update Rule:**\n",
    "For cluster $j$ with previous centroid $\\mu_j^{(t-1)}$ and count $N_j^{(t-1)}$, and batch contribution $(S_j^{(t)}, n_j^{(t)})$, the updated values are:\n",
    "$$\n",
    "\\mu_j^{(t)} = \\frac{\\lambda N_j^{(t-1)} \\mu_j^{(t-1)} + S_j^{(t)}}{\\lambda N_j^{(t-1)} + n_j^{(t)}}\n",
    "$$\n",
    "$$\n",
    "N_j^{(t)} = \\lambda N_j^{(t-1)} + n_j^{(t)}\n",
    "$$\n",
    "4. **Streaming loop:** Repeat for each micro‐batch. This yields a one‐pass, incremental clustering that can adapt to drifting distributions.\n",
    "\n",
    "### Differences Between Batch and Streaming\n",
    "\n",
    "- **Data access:** Batch K‐Means revisits all points until convergence; Streaming K‐Means processes each point exactly once (per micro‐batch) and updates centroids on the fly.  \n",
    "- **Adaptivity:** Batch K‐Means finds a static partition for a fixed dataset; Streaming K‐Means can adapt to distributional changes by setting $\\lambda < 1$.  \n",
    "- **Computation:** Batch requires multiple passes (expensive on large/distributed data); Streaming demands only one pass per batch and constant memory for centroids.  \n",
    "- **Use cases:** Batch suits offline analysis when data is static; Streaming K‐Means is ideal for real‐time clustering, online analytics, and concept‐drift scenarios (e.g., monitoring sensor readings, clickstreams, or bike‐sharing usage over time).\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "The following Python libraries are used throughout this streaming pipeline:\n",
    "\n",
    "- **pandas** (`import pandas as pd`):  \n",
    "  For loading and manipulating the CSV dataset in a tabular format.\n",
    "\n",
    "- **numpy** (`import numpy as np`):  \n",
    "  For efficient numerical computations, such as vector math and array transformations.\n",
    "\n",
    "- **matplotlib.pyplot** (`import matplotlib.pyplot as plt`):  \n",
    "  For plotting Batch SSE, Test SSE, and centroid trajectory visualizations.\n",
    "\n",
    "- **sklearn.preprocessing.StandardScaler** (`from sklearn.preprocessing import StandardScaler`):  \n",
    "  To normalize the input features (mean = 0, std = 1) before clustering.\n",
    "\n",
    "- **pyspark.SparkContext** (`from pyspark import SparkContext`):  \n",
    "  To initialize the Spark environment and create RDDs for distributed processing.\n",
    "\n",
    "- **pyspark.streaming.StreamingContext** (`from pyspark.streaming import StreamingContext`):  \n",
    "  To simulate a streaming context where RDDs are processed in micro-batches.\n",
    "\n",
    "- **pyspark.mllib.clustering.StreamingKMeans** (`from pyspark.mllib.clustering import StreamingKMeans`):  \n",
    "  For performing streaming K-Means clustering with model updates on each micro-batch.\n",
    "\n",
    "- **pyspark.mllib.linalg.Vectors** (`from pyspark.mllib.linalg import Vectors`):  \n",
    "  To convert NumPy arrays into Spark-compatible dense vector format for use in clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb69552",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TB\n",
    "\n",
    "subgraph SP[Setup Streaming]\n",
    "SA\n",
    "end\n",
    "\n",
    "SA[Staring Spark]\n",
    "\n",
    "subgraph DP[Data Preparation]\n",
    "LD\n",
    "PD\n",
    "SD\n",
    "end\n",
    "\n",
    "LD[Load Data]\n",
    "PD[Preprocess Data]\n",
    "SD[Split Data]\n",
    "\n",
    "subgraph SK[Streaming Kmeans]\n",
    "MI\n",
    "CR\n",
    "end\n",
    "\n",
    "MI[Model Initialization]\n",
    "CR[Create RDD Queue]\n",
    "\n",
    "subgraph SU[Streaming Update]\n",
    "CC\n",
    "AC\n",
    "TR\n",
    "end\n",
    "CC[Clustering Costs]\n",
    "AC[Assign Clusters]\n",
    "TR[Stream and Train]\n",
    "\n",
    "subgraph SC[Streaming Context]\n",
    "RS\n",
    "end\n",
    "RS[Run Streaming]\n",
    "\n",
    "subgraph EV[Evaluation and Visualization]\n",
    "RR\n",
    "PB\n",
    "PT\n",
    "PC\n",
    "end\n",
    "RR[Collect Results]\n",
    "PB[Plot Batch SSE]\n",
    "PT[Plot Test SSE]\n",
    "PC[Plot Centroid Trajectory]\n",
    "\n",
    "subgraph SB[Stopping Spark]\n",
    "ST\n",
    "end\n",
    "ST[Stop Spark]\n",
    "\n",
    "SA --> LD\n",
    "LD --> PD\n",
    "PD --> SD\n",
    "SD --> MI\n",
    "MI --> CR\n",
    "CR --> CC\n",
    "CC --> AC\n",
    "AC --> TR\n",
    "TR --> RS\n",
    "RS --> RR\n",
    "RR --> PB\n",
    "RR --> PT\n",
    "RR --> PC\n",
    "PB --> ST\n",
    "PT --> ST\n",
    "PC --> ST\n",
    "classDef titleClass white-space:nowrap,z-index:2,color:red;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66067ea4",
   "metadata": {},
   "source": [
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294261f",
   "metadata": {},
   "source": [
    "# Index\n",
    "- [Task SP: Setup Streaming](#task-SP)\n",
    "  - [Subtask SP.1: Staring Spark](#task-SP-subtask-1)\n",
    "- [Task DP: Data Preparation](#task-DP)\n",
    "  - [Subtask DP.1: Load Data](#task-DP-subtask-1)\n",
    "  - [Subtask DP.2: Preprocess Data](#task-DP-subtask-2)\n",
    "  - [Subtask DP.3: Split Data](#task-DP-subtask-3)\n",
    "- [Task SK: Streaming Kmeans](#task-SK)\n",
    "  - [Subtask SK.1: Model Initialization](#task-SK-subtask-1)\n",
    "  - [Subtask SK.2: Create RDD Queue](#task-SK-subtask-2)\n",
    "- [Task SU: Streaming Update](#task-SU)\n",
    "  - [Subtask SU.1: Clustering Costs](#task-SU-subtask-1)\n",
    "  - [Subtask SU.2: Assign Clusters](#task-SU-subtask-2)\n",
    "  - [Subtask SU.3: Stream and Train](#task-SU-subtask-3)\n",
    "- [Task SC: Streaming Context](#task-SC)\n",
    "  - [Subtask SC.1: Run Streaming](#task-SC-subtask-1)\n",
    "- [Task EV: Evaluation and Visualization](#task-EV)\n",
    "  - [Subtask EV.1: Collect Results](#task-EV-subtask-1)\n",
    "  - [Subtask EV.2: Plot Batch SSE](#task-EV-subtask-2)\n",
    "  - [Subtask EV.3: Plot Test SSE](#task-EV-subtask-3)\n",
    "  - [Subtask EV.4: Plot Centroid Trajectory](#task-EV-subtask-4)\n",
    "- [Task SB: Stopping Spark](#task-SB)\n",
    "  - [Subtask SB.1: Stop Spark](#task-SB-subtask-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082d58c",
   "metadata": {},
   "source": [
    "### Task 1: Setup Streaming <a id='task-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1027bc",
   "metadata": {},
   "source": [
    "Initialize the SparkSession and SparkContext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc7401",
   "metadata": {},
   "source": [
    "#### Subtask 1.1: Staring Spark <a id='task-1-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdb3e713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.022817Z",
     "start_time": "2025-12-01T23:42:59.983158Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.linalg import Vectors"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "c35ae94a-01f1-4847-a3d8-98cda7210df3",
   "metadata": {},
   "source": [
    "Initialize a PySpark environment by building a SparkSession, which serves as the primary entry point for DataFrame and Dataset APIs.\n",
    "A SparkSession automatically creates or reuses an underlying SparkContext (sc), which manages the cluster resources, job scheduling, and distributed execution of RDD transformations and actions.\n",
    "\n",
    "- **SparkSession (spark):**  \n",
    "  Acts as the unified entry point for reading data, creating DataFrames, and executing SQL queries. It encapsulates configuration details (master URL, application name, memory settings, etc.) and provides convenient methods (`read`, `createDataFrame`, `sql`, etc.) for high‐level data processing.\n",
    "\n",
    "- **SparkContext (sc):**  \n",
    "  Under the hood, SparkSession instantiates a SparkContext that coordinates with the cluster manager (e.g., local threads, YARN, or Mesos). SparkContext is responsible for:\n",
    "  1. **Resource Allocation:** Requesting executors or worker threads across the cluster.  \n",
    "  2. **Task Scheduling:** Breaking down RDD transformations into stages and tasks, then distributing them to executors.  \n",
    "  3. **Communication:** Managing shuffle and broadcast of variables (e.g., broadcasted centroids) for efficient distributed computation.\n",
    "\n",
    "By calling `SparkSession.builder.appName(app_name).getOrCreate()`, we bootstrap the Spark application, register the app with the cluster manager, and return both:\n",
    "  - `sc`: a SparkSession object for DataFrame and SQL operations  \n",
    "  - `ssc`: the corresponding SparkContext for lower‐level RDD manipulations  \n",
    "This setup is essential for the downstream streaming K‐Means pipeline, as we use SparkContext to create RDDs, broadcast variables (centroids), and perform distributed aggregate operations on micro‐batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "40792e2b-688f-44cb-b85d-827a8bfdbdf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.046408Z",
     "start_time": "2025-12-01T23:43:00.029466Z"
    }
   },
   "source": [
    "def setup_streaming_context(app_name=\"StreamingKMeansExample\", batch_duration=1):\n",
    "    \"\"\"Create SparkSession and SparkContext\n",
    "    input: app_name, str, Name of the Spark application\n",
    "    input: batch_duration, int, Duration of each micro-batch in the streaming context, Default: 1\n",
    "    output: sc, SparkContext, SparkContext used for creating RDDs and managing Spark jobs\n",
    "    output: ssc, StreamingContext, PySpark Streaming context for processing RDD queue\n",
    "    \"\"\"\n",
    "\n",
    "    sc = SparkContext(appName=app_name)\n",
    "    ssc = StreamingContext(sc, batchDuration=batch_duration)\n",
    "\n",
    "    return sc, ssc"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "88d62edd",
   "metadata": {},
   "source": [
    "### Task 2: Data Preparation <a id='task-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd55ecd",
   "metadata": {},
   "source": [
    "Load, split, preprocess the bike‐sharing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f086b7",
   "metadata": {},
   "source": [
    "#### Subtask 2.1: Load Data <a id='task-2-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e81e4",
   "metadata": {},
   "source": [
    "Use `pd.read_csv()` to load the CSV into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "353f84fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.073982Z",
     "start_time": "2025-12-01T23:43:00.046408Z"
    }
   },
   "source": [
    "def load_raw_data(file_path):\n",
    "    \"\"\"Read CSV into pandas DataFrame\n",
    "    input: file_path, str, Path to the CSV dataset\n",
    "    output: df, pandas DataFrame, Raw data loaded from CSV\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "2b6fc3f3",
   "metadata": {},
   "source": [
    "#### Subtask 2.2: Preprocess Data <a id='task-2-subtask-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0eace",
   "metadata": {},
   "source": [
    "Select the feature columns: \n",
    "`[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"casual\", \"registered\"]` from the raw DataFrame,\n",
    "fill missing values with 0, and standardize them using `sklearn.preprocessing.StandardScaler` \n",
    "to produce a normalized NumPy array `X_scaled`.\n",
    "\n",
    "This transformation ensures that each feature has zero mean and unit variance, \n",
    "which improves clustering quality and stabilizes centroid updates in K-Means.\n",
    "\n",
    "**Example Before Preprocessing (actual rows from hour.csv):**\n",
    "\n",
    "| temp  | atemp  | hum  | windspeed | casual | registered |\n",
    "|-------|--------|------|-----------|--------|------------|\n",
    "| 0.24  | 0.2879 | 0.81 | 0.0000    | 3      | 13         |\n",
    "| 0.22  | 0.2727 | 0.80 | 0.0000    | 8      | 32         |\n",
    "| 0.22  | 0.2727 | 0.80 | 0.0000    | 5      | 27         |\n",
    "\n",
    "**Example After Preprocessing (X_scaled NumPy Array):**\n",
    "\n",
    "```\n",
    "array([[-0.0923,  0.1441,  0.8701, -0.5546, -1.1338, -1.2186],\n",
    "      [-0.4536, -0.2517,  0.8327, -0.5546, -0.0882,  0.0565],\n",
    "      [-0.4536, -0.2517,  0.8327, -0.5546, -0.6110, -0.3602]])\n",
    "```\n",
    "\n",
    "Each row in `X_scaled` corresponds to a standardized feature vector,\n",
    "ready for micro-batching and StreamingKMeans clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "758d2337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.092978Z",
     "start_time": "2025-12-01T23:43:00.077207Z"
    }
   },
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Assemble feature vector\n",
    "    input: df, pandas DataFrame, Raw data loaded from CSV\n",
    "    output: X_scaled, ndarray, Normalized feature matrix using StandardScaler\n",
    "    \"\"\"\n",
    "\n",
    "    features = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered']\n",
    "    \n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    X = df[features].fillna(0).values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "    return X_scaled"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "4b1e9faa",
   "metadata": {},
   "source": [
    "#### Subtask 2.3: Split Data <a id='task-2-subtask-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5fb22",
   "metadata": {},
   "source": [
    "Split the standardized feature matrix `X_scaled` into training and test sets\n",
    "using a simple index-based slice.\n",
    "\n",
    "The `ratio` parameter defines the proportion of data to assign to the training set (default 0.8).\n",
    "This operation is performed using NumPy slicing to ensure fast, local splitting\n",
    "before converting to Spark RDDs.\n",
    "\n",
    "**Split Logic:**\n",
    "- Let N = number of rows in X_scaled  \n",
    "- Let split_index = floor(ratio × N)  \n",
    "- Assign rows 0 to (split_index - 1) to `train_X`  \n",
    "- Assign rows split_index to (N - 1) to `test_X`\n",
    "\n",
    "This results in:\n",
    "- `train_X`: 80% of the rows used for streaming training\n",
    "- `test_X`: 20% held out for evaluating test SSE after each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e645ca46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.115183Z",
     "start_time": "2025-12-01T23:43:00.102662Z"
    }
   },
   "source": [
    "def train_test_split(X_scaled, ratio=0.8):\n",
    "    \"\"\"Train/test split (80/20)\n",
    "    input: X_scaled, ndarray, Normalized feature matrix using StandardScaler\n",
    "    input: ratio, float, Train-test split ratio, Default: 0.8\n",
    "    output: train_X, ndarray, First 80 percent of X scaled used for training\n",
    "    output: test_X, ndarray, Last 20 percent of X scaled used for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    split_index = int(len(X_scaled) * ratio)\n",
    "\n",
    "    train_X = X_scaled[:split_index]\n",
    "    test_X = X_scaled[split_index:]\n",
    "\n",
    "    return train_X, test_X"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "47a9415a",
   "metadata": {},
   "source": [
    "### Task 3: Streaming Kmeans <a id='task-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb32c94",
   "metadata": {},
   "source": [
    "This block covers two essential subtasks before streaming can begin:\n",
    "\n",
    "1. **Model Initialization (MI):**  \n",
    "  We create the `StreamingKMeans` model with a specified number of clusters, decay factor, and randomly initialized centroids. This provides the initial state for learning.\n",
    "\n",
    "1. **RDD Queue Creation (CR):**  \n",
    "  We simulate streaming by dividing the training data into multiple micro-batches and converting each batch into a Spark RDD. These are queued and processed one at a time.\n",
    "\n",
    "Together, these subtasks prepare both the model and its input stream, enabling incremental updates as each batch arrives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94d0a0",
   "metadata": {},
   "source": [
    "#### Subtask 3.1: Model Initialization <a id='task-3-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5976c7a",
   "metadata": {},
   "source": [
    "This subtask sets up a `StreamingKMeans` model using PySpark’s `mllib.clustering` module.\n",
    "\n",
    "Before training begins, we must instantiate the model with:\n",
    "- A chosen number of clusters (`k`)\n",
    "- A decay factor (`decay_factor`) that controls how much past batches influence the update\n",
    "- Random initial cluster centers (via `np.random.randn`) of shape (`k`, `dim`)\n",
    "- Uniform weights (ones) assigned to all clusters equally\n",
    "\n",
    "The random seed ensures that initial center generation is deterministic and reproducible.\n",
    "\n",
    "Behind the scenes:\n",
    "- `StreamingKMeans.setInitialCenters()` accepts a matrix of cluster centers and a vector of weights.\n",
    "- These are used to compute a weighted centroid update as each micro-batch arrives.\n",
    "- The decay factor acts like a momentum term, giving less importance to older data.\n",
    "\n",
    "⚠️ **Note:**\n",
    "- Always set `k = 3` as per assignment instructions.\n",
    "- To get the correct `dim`, pass `X_scaled.shape[1]` — this ensures centroids match the number of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5145968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.138726Z",
     "start_time": "2025-12-01T23:43:00.122954Z"
    }
   },
   "source": [
    "def initialize_model(k, dim, decay_factor=0.8, seed=42):\n",
    "    \"\"\"reate StreamingKMeans model with random centers\n",
    "    input: k, int, Number of clusters for KMeans\n",
    "    input: dim, int, Dimensionality of the input feature vectors\n",
    "    input: decay_factor, float, Streaming decay factor for exponential forgetting, Default: 0.8\n",
    "    input: seed, int, Random seed for initializing cluster centers (default 42), Default: 42\n",
    "    output: model, StreamingKMeans, Streaming KMeans model that gets updated batch by batch\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    centers = np.random.randn(k, dim)\n",
    "    weights = np.ones(k)\n",
    "\n",
    "    model = StreamingKMeans(k=k, decayFactor=decay_factor)\n",
    "    model = model.setInitialCenters(centers, weights)\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "07effdbd",
   "metadata": {},
   "source": [
    "#### Subtask 3.2: Create RDD Queue <a id='task-3-subtask-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbde7dd",
   "metadata": {},
   "source": [
    "In streaming applications, data arrives in small chunks over time. Since we're using static input (like a full NumPy array), this function manually breaks the `train_X` dataset into a series of fixed-size mini-batches. Each mini-batch is converted into a Spark RDD and added to a queue that mimics a real-time stream.\n",
    "\n",
    "Here's what happens:\n",
    "- The training data is traversed in steps of `batch_size`.\n",
    "- Each subset of rows is extracted as a batch.\n",
    "- That batch is converted into an RDD using the SparkContext.\n",
    "- Each data point in the RDD is wrapped as a `DenseVector`, which is the format required by Spark MLlib clustering models.\n",
    "- These RDDs are appended to a list (`rdd_queue`) that will be fed to the streaming pipeline via `ssc.queueStream(...)`.\n",
    "\n",
    "This allows us to simulate how StreamingKMeans would behave with live data in production.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose your training data `train_X` looks like this:\n",
    "```\n",
    "[\n",
    "  [0.1, 0.2, 0.3],\n",
    "  [0.4, 0.5, 0.6],\n",
    "  [0.7, 0.8, 0.9],\n",
    "  [1.0, 1.1, 1.2]\n",
    "]\n",
    "```\n",
    "\n",
    "If you use `batch_size = 2`, the function will return a queue of two RDDs:\n",
    "- Batch 1 contains the first two vectors\n",
    "- Batch 2 contains the remaining two vectors\n",
    "\n",
    "Each of these is processed sequentially by the Spark Streaming engine to simulate incoming data batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef84a162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.162604Z",
     "start_time": "2025-12-01T23:43:00.146950Z"
    }
   },
   "source": [
    "def create_rdd_queue(sc, train_X, batch_size):\n",
    "    \"\"\"Split train array into micro-batches and convert to RDDs\n",
    "    input: sc, SparkContext, SparkContext used for creating RDDs and managing Spark jobs\n",
    "    input: train_X, ndarray, First 80 percent of X scaled used for training\n",
    "    input: batch_size, int, Number of points in each micro-batch\n",
    "    output: rdd_queue, list[RDD[DenseVector]], List of RDDs simulating incoming streaming batches\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    rdd_queue = []\n",
    "    for i in range(0, len(train_X), batch_size):\n",
    "        batch = train_X[i:i + batch_size]\n",
    "        rdd = sc.parallelize(batch).map(Vectors.dense)\n",
    "        rdd_queue.append(rdd)\n",
    "\n",
    "\n",
    "    return rdd_queue"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "5eae4900",
   "metadata": {},
   "source": [
    "### Task 4: Streaming Update <a id='task-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f2acf",
   "metadata": {},
   "source": [
    "The **Streaming Update** phase is where the clustering model continuously adapts to new incoming micro-batches. As each mini-batch is processed, the model incrementally updates its cluster centroids based on the most recent data.\n",
    "\n",
    "This stage ensures that the algorithm:\n",
    "- Reacts to changes in data distribution over time (concept drift)\n",
    "- Prioritizes newer data using a decay factor\n",
    "- Records key performance metrics (like Batch SSE and Test SSE)\n",
    "- Tracks the evolution of centroids for visualization and analysis\n",
    "\n",
    "By the end of this phase, we have a dynamically updated clustering model and a full record of its performance and centroid movement over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ff154",
   "metadata": {},
   "source": [
    "#### Subtask 4.1: Clustering Costs <a id='task-4-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6f57d",
   "metadata": {},
   "source": [
    "This function computes the **total clustering cost** — the sum of squared distances between each point in the dataset and its **nearest centroid**.\n",
    "\n",
    "Make sure to use np.array for calculations\n",
    "\n",
    "##### What It Does:\n",
    "\n",
    "For a given set of `centers` (NumPy arrays representing the cluster centroids) and a dataset `data` (2D NumPy array), the function:\n",
    "\n",
    "1. Iterates over each data point $x$ in `data`.\n",
    "2. For each point, calculates the squared Euclidean distance $\\|x - \\mu_j\\|^2$ to all centroids $\\mu_j$.\n",
    "3. Selects the smallest such distance.\n",
    "4. Adds it to a running sum representing the **clustering cost**.\n",
    "\n",
    "##### Formula:\n",
    "\n",
    "For every data point $x \\in \\mathbb{R}^d$, and $k$ cluster centers $\\{\\mu_1, \\mu_2, \\dots, \\mu_k\\}$, we compute:\n",
    "\n",
    "$$\n",
    "\\text{cost} = \\sum_{x \\in \\text{data}} \\min_j \\|x - \\mu_j\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\|x - \\mu_j\\|^2 = \\sum_{i=1}^d (x_i - \\mu_{j,i})^2$ is the squared Euclidean distance.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Suppose:\n",
    "- `centers = [[1.0, 3.0], [5.0, 1.0], [2.5, 4.5]]`\n",
    "- `data = [[2.0, 4.0], [4.0, 2.0]]`\n",
    "\n",
    "- For point $[2.0, 4.0]$:\n",
    "  - Distance to center 0: $(2 - 1)^2 + (4 - 3)^2 = 1 + 1 = 2$\n",
    "  - To center 1: $(2 - 5)^2 + (4 - 1)^2 = 9 + 9 = 18$\n",
    "  - To center 2: $(2 - 2.5)^2 + (4 - 4.5)^2 = 0.25 + 0.25 = 0.5$\n",
    "  - Nearest distance = **0.5**\n",
    "\n",
    "- For point $[4.0, 2.0]$:\n",
    "  - Closest center is 1 → squared distance = $(4 - 5)^2 + (2 - 1)^2 = 1 + 1 = 2$\n",
    "\n",
    "**Total Cost** = $0.5 + 2 = 2.5$\n",
    "\n",
    "This output helps us evaluate the quality of the current clustering: **lower cost indicates better cohesion**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "23f55d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.194101Z",
     "start_time": "2025-12-01T23:43:00.171657Z"
    }
   },
   "source": [
    "def clustering_cost(centers, data):\n",
    "    \"\"\"Total squared error from nearest centroids\n",
    "    input: centers, list[ndarray], Final centroids output from the trained model\n",
    "    input: data, ndarray, Array of input vectors to compute clustering cost against given centers\n",
    "    output: cost, float, Final test SSE cost with learned cluster centers\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    centers_arr = np.vstack(centers)\n",
    "    diff = data[:, None, :] - centers_arr[None, :, :]\n",
    "    dists_sq = np.sum(diff ** 2, axis=2)\n",
    "\n",
    "    cost = float(np.sum(np.min(dists_sq, axis=1)))\n",
    "\n",
    "    return cost"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "bf5c43dc",
   "metadata": {},
   "source": [
    "#### Subtask 4.2: Assign Clusters <a id='task-4-subtask-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008df588",
   "metadata": {},
   "source": [
    "This function assigns each point in the dataset to the nearest cluster center. It is typically used to produce final cluster assignments after training, or to evaluate how the model partitions a given dataset.\n",
    "\n",
    "##### What It Does:\n",
    "\n",
    "For every data point in the input `data`:\n",
    "- Compute the squared Euclidean distance to each centroid in `centers`\n",
    "- Identify the centroid with the smallest distance using `np.argmin`\n",
    "- Assign the point to that cluster (by index)\n",
    "\n",
    "The result is a list of tuples, where each element is:\n",
    "- The index of the nearest centroid\n",
    "- The original point (rounded) assigned to that centroid\n",
    "\n",
    "##### Formula:\n",
    "\n",
    "For each point $x$ and cluster centers $\\{\\mu_1, \\mu_2, \\dots, \\mu_k\\}$:\n",
    "\n",
    "$$\n",
    "j^* = \\arg\\min_j \\|x - \\mu_j\\|^2\n",
    "$$\n",
    "\n",
    "The value $j^*$ is stored as the assignment for point $x$.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Suppose:\n",
    "- `centers = [[1.0, 3.0], [5.0, 1.0], [2.5, 4.5]]`\n",
    "- `data = [[2.0, 4.0], [4.0, 2.0]]`\n",
    "\n",
    "For $[2.0, 4.0]$, closest center is at index 2  \n",
    "For $[4.0, 2.0]$, closest center is at index 1  \n",
    "\n",
    "Output: `[(2, [2.0, 4.0]), (1, [4.0, 2.0])]`\n",
    "\n",
    "These assignments indicate the cluster index each point belongs to.\n",
    "\n",
    "**Note:**  \n",
    "Use `np.argmin` over the list of distances to efficiently determine the closest centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d616d238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.230277Z",
     "start_time": "2025-12-01T23:43:00.203972Z"
    }
   },
   "source": [
    "def assign_clusters(centers, data):\n",
    "    \"\"\"Assign each point to nearest centroid\n",
    "    input: centers, list[ndarray], Final centroids output from the trained model\n",
    "    input: data, ndarray, Array of input vectors to compute clustering cost against given centers\n",
    "    output: assignments, list[tuple[int, list[float]]], Final assignment of test points to nearest clusters\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    centers_arr = np.vstack(centers)\n",
    "    diff = data[:, None, :] - centers_arr[None, :, :]\n",
    "    dists_sq = np.sum(diff ** 2, axis=2)\n",
    "\n",
    "    nearest_idx = np.argmin(dists_sq, axis=1)\n",
    "    assignments = [(int(idx), data[i].tolist()) for i, idx in enumerate(nearest_idx)]\n",
    "\n",
    "    return assignments"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "2ef1ddfd",
   "metadata": {},
   "source": [
    "#### Subtask 4.3: Stream and Train <a id='task-4-subtask-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133d4ff",
   "metadata": {},
   "source": [
    "This function connects the streaming data pipeline to the clustering model and collects evaluation metrics over time.\n",
    "\n",
    "##### What It Does:\n",
    "\n",
    "It defines a **nested function** called `track_stats(time, rdd)` which gets invoked automatically by Spark for each mini-batch (as an RDD). This allows us to analyze each batch during streaming without materializing the entire dataset in memory.\n",
    "\n",
    "**Inside `track_stats`:**\n",
    "- Collects the RDD and checks if it contains any data.\n",
    "- Retrieves the current cluster centers from the model.\n",
    "- Computes:\n",
    "  - **Batch SSE**: clustering error on the incoming batch.\n",
    "  - **Test SSE**: clustering error on the held-out test set.\n",
    "- Stores these statistics for plotting or analysis.\n",
    "- Appends the latest cluster centers to a history list for trajectory visualization.\n",
    "\n",
    "Finally, the outer function:\n",
    "- Attaches `track_stats` to every batch in the stream using `foreachRDD`.\n",
    "- Triggers model training on the same stream using `.trainOn(...)`.\n",
    "\n",
    "##### Why This Matters:\n",
    "\n",
    "- Spark executes in a distributed, lazy fashion. To inspect evolving metrics like SSE over time, we must collect relevant batch-level statistics inside an RDD processing function.\n",
    "- Tracking both batch and test errors gives insight into convergence and concept drift during training.\n",
    "- Capturing centroids over time enables trajectory plots of how clusters shift as more data arrives.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Suppose the first micro-batch contains:\n",
    "```\n",
    "RDD = [[0.1, 0.2], [0.5, 0.6]]\n",
    "```\n",
    "And the current cluster centers are:\n",
    "```\n",
    "[[0.0, 0.0], [1.0, 1.0]]\n",
    "```\n",
    "Then we compute:\n",
    "- Distance of each point to each center\n",
    "- Assign nearest center\n",
    "- Compute sum of squared distances = Batch SSE\n",
    "- Also compute SSE on the fixed `test_X` dataset\n",
    "\n",
    "##### Pseudocode:\n",
    "\n",
    "```python\n",
    "def attach_stream_and_train(stream, model, test_X, ...):\n",
    "\n",
    "    def track_stats(time, rdd):\n",
    "        if rdd has points:\n",
    "            centers = model.latestModel().centers\n",
    "            record current centers\n",
    "            compute batch SSE using clustering_cost(centers, batch)\n",
    "            compute test SSE using clustering_cost(centers, test_X)\n",
    "            append both to tracking lists\n",
    "\n",
    "    stream.foreachRDD(track_stats)\n",
    "    model.trainOn(stream)\n",
    "```\n",
    "\n",
    "This is a key monitoring step that ensures the streaming K-Means is not just running, but learning and adapting effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e94b869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.268163Z",
     "start_time": "2025-12-01T23:43:00.239472Z"
    }
   },
   "source": [
    "def attach_stream_and_train(stream, model, test_X, batch_sse_list, test_sse_list, centroids_history):\n",
    "    \"\"\"Track and update clustering performance over time\n",
    "    input: stream, DStream, Streaming queue of RDD batches to be processed\n",
    "    input: model, StreamingKMeans, Streaming KMeans model that gets updated batch by batch\n",
    "    input: test_X, ndarray, Last 20 percent of X scaled used for evaluation\n",
    "    input: batch_sse_list, list[float], Per-batch sum of squared errors over training batches\n",
    "    input: test_sse_list, list[float], SSE of the current model on the test data after each batch\n",
    "    input: centroids_history, list[list[ndarray]], History of cluster centers after each micro-batch\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    def track_stats(time, rdd):\n",
    "        if rdd.isEmpty():\n",
    "            return\n",
    "\n",
    "        batch = np.array(rdd.collect())\n",
    "        centers = model.latestModel().centers\n",
    "\n",
    "        centroids_history.append([c.copy() for c in centers])\n",
    "        batch_sse = clustering_cost(centers, batch)\n",
    "        batch_sse_list.append(batch_sse)\n",
    "        test_sse = clustering_cost(centers, test_X)\n",
    "        test_sse_list.append(test_sse)\n",
    "\n",
    "\n",
    "    stream.foreachRDD(track_stats)\n",
    "    model.trainOn(stream)\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "77e676c6",
   "metadata": {},
   "source": [
    "### Task 5: Streaming Context <a id='task-5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21c324",
   "metadata": {},
   "source": [
    "This module controls the lifecycle of the **Spark Streaming job**. It starts the `StreamingContext` to process micro-batches in real time and stops it once processing is complete or the timeout is reached.\n",
    "\n",
    "Properly managing the `StreamingContext` ensures the streaming pipeline runs smoothly and exits gracefully — preventing resource leaks or dangling jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2367873",
   "metadata": {},
   "source": [
    "#### Subtask 5.1: Run Streaming <a id='task-5-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adbeeb",
   "metadata": {},
   "source": [
    "Launches the Spark Streaming process using:\n",
    "\n",
    "- `ssc.start()` to begin reading and processing RDD batches from the stream\n",
    "- `ssc.awaitTerminationOrTimeout(timeout)` to keep it alive for a fixed duration\n",
    "- `ssc.stop(stopSparkContext=False)` to stop the streaming context once the timeout is reached or the job is completed\n",
    "\n",
    "The `timeout` parameter controls how long (in seconds) the streaming job stays active. Use `timeout = 80` or a value where all batches results are processed,  by default unless you intend to stream indefinitely.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ae986f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.287746Z",
     "start_time": "2025-12-01T23:43:00.279119Z"
    }
   },
   "source": [
    "def run_streaming(ssc, timeout=80):\n",
    "    \"\"\"Start and run StreamingContext\n",
    "    input: ssc, StreamingContext, PySpark Streaming context for processing RDD queue\n",
    "    input: timeout, int, Duration (in seconds) to run the streaming job before automatic termination\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTerminationOrTimeout(timeout)\n",
    "    ssc.stop(stopSparkContext=False)\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "5f30a9d8-6796-436d-8fd7-c2033db28e7e",
   "metadata": {},
   "source": [
    "#### Subtask 5.2: Stop Streaming <a id='task-5-subtask-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6c4b1-1f82-4ebb-aaca-74245e15ad16",
   "metadata": {},
   "source": [
    "Gracefully halts the streaming execution using `ssc.stop(...)`.\n",
    "\n",
    "This function ensures that all micro-batches currently in progress are completed, and then the streaming job is terminated. Use this after `run_streaming` to shut down streaming logic before stopping the Spark cluster itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c6ff704-258a-4c79-8902-280c4edf6bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.323081Z",
     "start_time": "2025-12-01T23:43:00.306199Z"
    }
   },
   "source": [
    "def stop_streaming(ssc):\n",
    "    \"\"\"Stop the StreamingContext\n",
    "    input: ssc, StreamingContext, PySpark Streaming context for processing RDD queue\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    ssc.stop(stopSparkContext=False)\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "72a47725",
   "metadata": {},
   "source": [
    "### Task 6: Evaluation and Visualization <a id='task-6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7fe5c",
   "metadata": {},
   "source": [
    "After the streaming K-Means pipeline completes, this phase evaluates the clustering performance and visualizes the learning dynamics across micro-batches.\n",
    "\n",
    "#### Core Objectives:\n",
    "\n",
    "- **Metric Collection**:  \n",
    "  Measure clustering quality at each step using:\n",
    "    - **Batch SSE**: Sum of squared distances within clusters on each micro-batch.\n",
    "    - **Test SSE**: SSE computed on a static hold-out test set to monitor generalization.\n",
    "\n",
    "- **Trend Visualization**:  \n",
    "  Plot the recorded metrics and centroid movements to understand how the model evolved:\n",
    "    1. **Batch SSE vs Batch Index** – Monitors convergence and clustering tightness per batch.\n",
    "    2. **Test SSE vs Batch Index** – Evaluates whether generalization is improving or degrading.\n",
    "    3. **Centroid Trajectories** – Visualizes how cluster centers shift over time, indicating convergence or concept drift.\n",
    "\n",
    "This phase helps:\n",
    "- Validate that the model is learning meaningful cluster structures.\n",
    "- Detect problems like poor convergence, instability, or overfitting.\n",
    "- Provide interpretable visuals for analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be3896",
   "metadata": {},
   "source": [
    "#### Subtask 6.1: Collect Results <a id='task-6-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760e6e9",
   "metadata": {},
   "source": [
    "This task gathers evaluation metrics for each micro-batch during streaming by tracking clustering performance.\n",
    "\n",
    "#### What It Does:\n",
    "\n",
    "- **Retrieve Model State**:  \n",
    "  Get the current centroids from the latest version of the streaming model using `model.latestModel().centers`.\n",
    "\n",
    "- **Track Centroid History**:  \n",
    "  Convert each centroid to a NumPy array and append it to `centroids_history` to monitor movement over time.\n",
    "\n",
    "- **Compute Evaluation Metrics**:\n",
    "  Calculate all five of the following:\n",
    "    1. **Final Cluster Centers** – list of lists\n",
    "    2. **Assignments** – predicted cluster index for each test point using `assign_clusters()`\n",
    "    3. **Final Test SSE** – clustering_cost on `test_X` with latest centers\n",
    "    4. **Batch SSE List** – historical SSE per batch\n",
    "    5. **Test SSE List** – SSE over test set per batch\n",
    "\n",
    "- **Use Helper Functions**:  \n",
    "  Call:\n",
    "    - `assign_clusters()` to match test points to nearest centroids.\n",
    "    - `clustering_cost()` to calculate SSE for both streaming batches and test data.\n",
    "\n",
    "#### Example Output:\n",
    "```json\n",
    "{\n",
    "  \"centers\": [\n",
    "    [0.1234, -0.5678, 0.3456],\n",
    "    [-0.8765, 0.4321, -0.1234],\n",
    "    [0.6789, 0.0123, 0.4567]\n",
    "  ],\n",
    "  \"assignments\": [1, 0, 2, 2, 1],\n",
    "  \"cost\": 12.34,\n",
    "  \"batch_sse_list\": [15.22, 14.11, 13.89, 12.76, 12.34],\n",
    "  \"test_sse_list\": [20.15, 18.43, 17.56, 13.88, 12.34]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "909a8ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.364123Z",
     "start_time": "2025-12-01T23:43:00.353551Z"
    }
   },
   "source": [
    "def collect_results(model, test_X, batch_sse_list, test_sse_list, centroids_history):\n",
    "    \"\"\"Sum of squared errors across streaming iterations\n",
    "    input: model, StreamingKMeans, Streaming KMeans model that gets updated batch by batch\n",
    "    input: test_X, ndarray, Last 20 percent of X scaled used for evaluation\n",
    "    input: batch_sse_list, list[float], Per-batch sum of squared errors over training batches\n",
    "    input: test_sse_list, list[float], SSE of the current model on the test data after each batch\n",
    "    input: centroids_history, list[list[ndarray]], History of cluster centers after each micro-batch\n",
    "    output: results, dict, Dictionary holding final output\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    centers = model.latestModel().centers\n",
    "    centers_list = [c.tolist() for c in centers]\n",
    "\n",
    "    assignments = assign_clusters(centers, test_X)\n",
    "\n",
    "    final_test_sse = clustering_cost(centers, test_X)\n",
    "\n",
    "    results = {\n",
    "        \"centers\": centers_list,\n",
    "        \"assignments\": assignments,\n",
    "        \"cost\": final_test_sse,\n",
    "        \"batch_sse_list\": batch_sse_list,\n",
    "        \"test_sse_list\": test_sse_list,\n",
    "        \"centroids_history\": centroids_history\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "2b49fc6e",
   "metadata": {},
   "source": [
    "#### Subtask 6.2: Plot Batch SSE <a id='task-6-subtask-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3642ee2",
   "metadata": {},
   "source": [
    "This function visualizes the convergence behavior of the model by plotting the \n",
    "**Sum of Squared Errors (SSE)** for each micro-batch.\n",
    "\n",
    "#### What it does:\n",
    "- Convert `batch_sse_list` into a DataFrame or use list indexing directly.\n",
    "- Plot a line graph of `batch_index` (x-axis) vs. `batch_sse` (y-axis).\n",
    "- Use appropriate axis labels and titles to highlight trends.\n",
    "\n",
    "This helps identify whether the model is learning over time — a decreasing SSE implies tighter clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "77172f54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.398697Z",
     "start_time": "2025-12-01T23:43:00.388447Z"
    }
   },
   "source": [
    "def plot_batch_sse(batch_sse_list):\n",
    "    \"\"\"Line‐plot of batch SSE over time\n",
    "    input: batch_sse_list, list[float], Per-batch sum of squared errors over training batches\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    batches = range(len(batch_sse_list))\n",
    "    plt.plot(batches, batch_sse_list)\n",
    "    plt.title(\"Batch SSE\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "e0fea5b1",
   "metadata": {},
   "source": [
    "#### Subtask 6.3: Plot Test SSE <a id='task-6-subtask-3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00552fa2",
   "metadata": {},
   "source": [
    "This function evaluates generalization performance of the model after each batch update.\n",
    "\n",
    "#### What it does:\n",
    "- Convert `test_sse_list` into a DataFrame or indexed list.\n",
    "- Plot a line graph of `batch_index` vs. `test_sse`.\n",
    "- Add axis labels, title, and grid to clearly communicate the trend.\n",
    "\n",
    "A rising test SSE with a falling batch SSE may indicate overfitting or poor generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d3e0762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.440243Z",
     "start_time": "2025-12-01T23:43:00.417724Z"
    }
   },
   "source": [
    "def plot_test_sse(test_sse_list):\n",
    "    \"\"\"Line‐plot of test SSE over time\n",
    "    input: test_sse_list, list[float], SSE of the current model on the test data after each batch\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "    batches = range(len(test_sse_list))\n",
    "    plt.plot(batches, test_sse_list)\n",
    "    plt.title(\"Test SSE\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "f1756468",
   "metadata": {},
   "source": [
    "#### Subtask 6.4: Plot Centroid Trajectory <a id='task-6-subtask-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a8ce6",
   "metadata": {},
   "source": [
    "This visualization shows how each cluster’s centroid **moves across streaming updates**, \n",
    "revealing adaptation and convergence over time.\n",
    "\n",
    "#### What it does:\n",
    "- For each cluster, extract its centroid's coordinates from all batches.\n",
    "- Plot the first two dimensions (e.g., features 0 and 1) as a 2D path.\n",
    "- Optionally, add arrows or markers to indicate direction of movement.\n",
    "\n",
    "This provides an intuitive view of whether centroids stabilize or continue shifting — a sign of concept drift.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fb8e9e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.475245Z",
     "start_time": "2025-12-01T23:43:00.446690Z"
    }
   },
   "source": [
    "def plot_centroid_trajectories(centroids_history):\n",
    "    \"\"\"2D projection of centroid paths\n",
    "    input: centroids_history, list[list[ndarray]], History of cluster centers after each micro-batch\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    if not centroids_history:\n",
    "        print(\"No centroid history recorded.\")\n",
    "        return\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    K = len(centroids_history[0])\n",
    "    for k in range(K):\n",
    "        \"\"\"__Your_Code_Here__\"\"\"\n",
    "        xs = [centroids_history[t][k][0] for t in range(len(centroids_history))]\n",
    "        ys = [centroids_history[t][k][1] for t in range(len(centroids_history))]\n",
    "        plt.plot(xs, ys, marker=\"o\", label=f\"Centroid {k}\")\n",
    "    plt.title(\"Centroid Trajectories (2D projection)\")\n",
    "    plt.xlabel(\"Dimension 0\")\n",
    "    plt.ylabel(\"Dimension 1\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "026c0155",
   "metadata": {},
   "source": [
    "### Task 7: Stopping Spark <a id='task-7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291d45c",
   "metadata": {},
   "source": [
    "Cleanly shut down the SparkSession and SparkContext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f63f4a",
   "metadata": {},
   "source": [
    "#### Subtask 7.1: Stop Spark <a id='task-7-subtask-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca896ea",
   "metadata": {},
   "source": [
    "Call `sc.stop()` to release resources and end the Spark application."
   ]
  },
  {
   "cell_type": "code",
   "id": "43487de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.497259Z",
     "start_time": "2025-12-01T23:43:00.484343Z"
    }
   },
   "source": [
    "def stop_spark(sc):\n",
    "    \"\"\"Stop the SparkSession\n",
    "    input: sc, SparkContext, SparkContext used for creating RDDs and managing Spark jobs\n",
    "    output: None\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"__Your_Code_Here__\"\"\"\n",
    "\n",
    "    sc.stop()\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:00.532335Z",
     "start_time": "2025-12-01T23:43:00.508587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))"
   ],
   "id": "764aeca99352bb1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "8ce7798d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:43:04.346053Z",
     "start_time": "2025-12-01T23:43:00.547627Z"
    }
   },
   "source": [
    "def main():\n",
    "    df = load_raw_data(\"streaming_kmeans_1.csv\")\n",
    "    X_scaled = preprocess_data(df)\n",
    "    train_X, test_X = train_test_split(X_scaled)\n",
    "\n",
    "    batch_sse_list, test_sse_list, centroids_history = [], [], []\n",
    "\n",
    "    sc, ssc = setup_streaming_context()\n",
    "    model = initialize_model(k=3, dim=X_scaled.shape[1], decay_factor=0.8)\n",
    "\n",
    "    rdd_queue = create_rdd_queue(sc, train_X, batch_size=100000)\n",
    "    stream = ssc.queueStream(rdd_queue)\n",
    "\n",
    "    attach_stream_and_train(stream, model, test_X, batch_sse_list, test_sse_list, centroids_history)\n",
    "    run_streaming(ssc, timeout=80)\n",
    "    stop_streaming(ssc)\n",
    "\n",
    "    results = collect_results(model, test_X, batch_sse_list, test_sse_list, centroids_history)\n",
    "\n",
    "    print(f\"\\n===== FINAL TEST SET SSE COST =====\")\n",
    "    print(f\"Cost = {round(results['cost'], 2)}\\n\")\n",
    "\n",
    "    print(\"Final Cluster Centers:\")\n",
    "    for idx, center in enumerate(results[\"centers\"]):\n",
    "        print(f\"Cluster {idx}: {center}\")\n",
    "\n",
    "    print(\"\\nFirst 10 Test Assignments:\")\n",
    "    for cluster, vec in results[\"assignments\"][:10]:\n",
    "        print(f\"Data point assigned to Cluster {cluster}: {vec}\")\n",
    "\n",
    "    print(\"\\nBatch SSE List:\", results[\"batch_sse_list\"])\n",
    "    print(\"Test SSE List:\", results[\"test_sse_list\"])\n",
    "    print(\"Centroid history length:\", len(results[\"centroids_history\"]))\n",
    "\n",
    "    plot_batch_sse(results[\"batch_sse_list\"])\n",
    "    plot_test_sse(results[\"test_sse_list\"])\n",
    "    plot_centroid_trajectories(results[\"centroids_history\"])\n",
    "\n",
    "    stop_spark(sc)\n",
    "\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 44\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Entry point\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 44\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[62], line 8\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m train_X, test_X \u001B[38;5;241m=\u001B[39m train_test_split(X_scaled)\n\u001B[0;32m      6\u001B[0m batch_sse_list, test_sse_list, centroids_history \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[1;32m----> 8\u001B[0m sc, ssc \u001B[38;5;241m=\u001B[39m \u001B[43msetup_streaming_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m initialize_model(k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, dim\u001B[38;5;241m=\u001B[39mX_scaled\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], decay_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m)\n\u001B[0;32m     11\u001B[0m rdd_queue \u001B[38;5;241m=\u001B[39m create_rdd_queue(sc, train_X, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100000\u001B[39m)\n",
      "Cell \u001B[1;32mIn[45], line 9\u001B[0m, in \u001B[0;36msetup_streaming_context\u001B[1;34m(app_name, batch_duration)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msetup_streaming_context\u001B[39m(app_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStreamingKMeansExample\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_duration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create SparkSession and SparkContext\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m    input: app_name, str, Name of the Spark application\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m    input: batch_duration, int, Duration of each micro-batch in the streaming context, Default: 1\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03m    output: sc, SparkContext, SparkContext used for creating RDDs and managing Spark jobs\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m    output: ssc, StreamingContext, PySpark Streaming context for processing RDD queue\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapp_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     ssc \u001B[38;5;241m=\u001B[39m StreamingContext(sc, batchDuration\u001B[38;5;241m=\u001B[39mbatch_duration)\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sc, ssc\n",
      "File \u001B[1;32m~\\PycharmProjects\\MD_SKM\\.venv1\\lib\\site-packages\\pyspark\\context.py:133\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 133\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m    136\u001B[0m                   conf, jsc, profiler_cls)\n",
      "File \u001B[1;32m~\\PycharmProjects\\MD_SKM\\.venv1\\lib\\site-packages\\pyspark\\context.py:327\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[1;32m--> 327\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    328\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[1;32m~\\PycharmProjects\\MD_SKM\\.venv1\\lib\\site-packages\\pyspark\\java_gateway.py:105\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    102\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[1;32m--> 105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[0;32m    108\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[1;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "af453657",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Spark Streaming K‐Means Documentation](https://spark.apache.org/docs/latest/ml-clustering.html#streaming-k-means)\n",
    "- [Streaming KMeans in Spark | Real-time Clustering with PySpark (YouTube)](https://www.youtube.com/watch?v=N3Mfo880K54)\n",
    "- [What is Streaming K-Means? (DSWorld)](https://dsworld.org/what-is-a-streaming-k-means/)\n",
    "- [Online K-Means Clustering of Large High-Dimensional Data Streams (ICDE 2017)](https://www.ece.iastate.edu/snt/files/2017/08/kmeans-ICDE-2017.pdf)\n",
    "- [Introducing Streaming K-Means in Spark 1.2 (Databricks Blog)](https://www.databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
